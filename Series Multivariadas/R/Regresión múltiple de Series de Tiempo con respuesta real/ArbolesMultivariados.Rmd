---
title: "Modelos de Árboles"
output: html_document
date: '2022-03-22'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Modelos de Árboles

Vamos a ver un ejmeplo de árboles binarios de regresión para predicción. 

```{r Arboles Binarios construccion de la base de datos}
require(tree)
require(SLBDD)
library(dplyr)


```


```{r Graficos y exploratorios}
require(quantilogram)
FREDMDApril19<-read.csv("FREDMDApril19.csv",header = TRUE)
x=as.matrix(FREDMDApril19)
require(MTS)
y=scale(x,scale=TRUE) ##Se estandarizan los datos para que sean dibujados
tdx=c(1:710/12+1960)  #creación del índice
MTSplot(y,tdx)

x[,c(6,7,8)] ###tomamos los estas tres series
#quantilogram::crossq.max.partial(x[,c(6,7,8)],c(0.9,0.9,0.9),10)
```


```{r Arboles 1}
require(tree)
Y=x[7:710,6] # variable dependiente o de respuesta univariada
X=cbind(x[6:709,],x[5:708,],x[4:707,],x[3:706,],x[2:705,],x[1:704,])
X1 <- scale(X,center=TRUE,scale=TRUE)
tree.fit <- tree(Y~.,data=data.frame(X1),split="deviance")
tree.fit

summary(tree.fit)

plot(tree.fit)
text(tree.fit,cex=0.5)


########División entrenamiento y prueba
dim(X1)
n_entre=600
X1.train=X1[1:n_entre,]
Y.train=Y[1:n_entre]

X1.test=X1[(n_entre+1):dim(X1)[1],]
Y.test=Y[(n_entre+1):dim(X1)[1]]
```
### Bosques Aletorios
Vamos a considerar el mismo ejemplo del modelo árboles anterior, en este caso tenemos $n=704$ observaciones y $k=732$ variables. Los predictores están estandarizados. El argumento ntree controla el número de árboles, mientras que mtry controla el número de predictores a seleccionarse en cada partición, es decir $g$, en la notación que manejamos. Por defecto, el crecimiento del árbol se deja bajo la condición que el número de datos puntuales no sean inferior a 5. Eso se puede cambiar usando el argumento $nodsize$ y $maxnodes$.  

```{r random forest}
require(randomForest)
set.seed(117)
rf.fit <- randomForest(X1.train,Y.train,ntree=500,mtry=20,importance=TRUE)
rf.fit


plot(rf.fit) ###Esta gráfica permite ver la disminución del ECM de predicción al incrementar el número de árboles.
```

```{r Importancia y Grácias de dependencia parcial}

dim(rf.fit$importance)

plot(1:732,rf.fit$importance[,2],ylab="mean decrease in MSE")   ##La gráfica muestra la disminución medio el EMC de pronóstico de cada predictor. 

varImpPlot(rf.fit)   ###Importancia de las variables

require(pdp)
require(vip)
vip(rf.fit, bar = FALSE, horizontal = FALSE, size = 1.5)
partialPlot(rf.fit,data.frame(cbind(Y.train,X1.train)),"USGOOD") ##representación gráfica del efecto marginal de una variable en la respuesta
partialPlot(rf.fit,data.frame(cbind(Y.train,X1.train)),"NDMANEMP") ##representación gráfica del efecto marginal de una variable en la respuesta
```

```{r Como cambian el error cuadrático medio con respecto a la número de predictores}
kinic=150
Kfinal=180
K=Kfinal-kinic
oob.err=double(K)
test.err=double(K)

##mtry is number of Variables randomly chosen at each split
i=1
for(mtry in kinic:Kfinal) 
{
  
  rf=randomForest(X1.train,Y.train,ntree=300,mtry=mtry,importance=TRUE)
  oob.err[i] = rf$mse[400] 
  
  pred<-predict(rf,X1.test) #Predictions on Test Set for each Tree
  test.err[i]= mean((Y.test - pred)^2) #Mean Squared Test Error
  
  cat(i," ") #printing the output to the console
  i=i+1
}
oob.err
test.err
```

```{r Gráfico}
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Error Cuadrático Medio",xlab="Número de Predictores Considerados en cada división")
#legend("top",legend=c("Error Intramuestra","Error en el conjunto de Prueba"),pch=19, col=c("red","blue"))
```


Tarea:Qué sucede si aumentamos el número de predictoras en las particiones?

Hacer la mejora de requerirse usando la metodología que se menciona enseguida!

Recuerde que en el archivo Arboles.Rmd de series univariadas se encuentra 


## Random Forest con remuestreo temporal 
La idea es usar el reposotorio en github https://github.com/hyanworkspace/rangerts
donde se está el paquete rangerts que permite usar random forest para el contexto de series de tiempo, el cual está basado en el paquete ranger.

```{r Rf temporal}
## devtools::install_github("hyanworkspace/rangerts", quiet = T) ## instalación del paquete
library(rangerts)
```

